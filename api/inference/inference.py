from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

MODEL_DIR = "../results/"

tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)

# ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì „í™˜
model.eval()

# ìƒ˜í”Œ ì…ë ¥
sample_text = "ì•ˆë…•í•˜ì„¸ìš”"

# í† í¬ë‚˜ì´ì €ë¡œ ì…ë ¥ í…ìŠ¤íŠ¸ í† í°í™”
inputs = tokenizer(
    sample_text,
    return_tensors="pt",  # PyTorch í…ì„œë¡œ ë°˜í™˜
    padding=True,         # íŒ¨ë”© ì¶”ê°€
    truncation=True,      # ë¬¸ì¥ ìë¥´ê¸°
    max_length=512        # ìµœëŒ€ ê¸¸ì´ ì œí•œ
)

# ëª¨ë¸ì— ì…ë ¥ ë°ì´í„° ì „ë‹¬
with torch.no_grad():  # ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ì§€ ì•ŠìŒ (í‰ê°€ ëª¨ë“œ)
    outputs = model(**inputs)
    logits = outputs.logits  # ë¶„ë¥˜ ê²°ê³¼ logits

# ê²°ê³¼ í™•ì¸
predicted_class = torch.argmax(logits, dim=1).item()  # ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í´ë˜ìŠ¤
print(f"ğŸ” ì…ë ¥ ë¬¸ì¥: {sample_text}")
print(f"âœ… ì˜ˆì¸¡ëœ í´ë˜ìŠ¤: {predicted_class}")